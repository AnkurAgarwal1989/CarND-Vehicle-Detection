{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import time\n",
    "from features import extract_features, calc_bin_spatial_features\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_pickle = pickle.load( open('Car_NoCar_LinearSVC.p', 'r' ) )\n",
    "svc = dist_pickle[\"svc\"]\n",
    "X_scaler = dist_pickle[\"scaler\"]\n",
    "orient = dist_pickle[\"orientations\"]\n",
    "pix_per_cell = dist_pickle[\"pix_per_cell\"]\n",
    "cell_per_block = dist_pickle[\"cell_per_block\"]\n",
    "spatial_size = dist_pickle[\"color_spatial_size\"]\n",
    "h_colorspace = dist_pickle[\"h_colorspace\"]\n",
    "s_colorspace = dist_pickle[\"s_colorspace\"]\n",
    "#hog_channel = dist_pickle[\"hog_channel\"]\n",
    "hog_channel = 'ALL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a single function that can extract features using hog sub-sampling and make predictions\n",
    "def find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, \n",
    "              spatial_size, cells_per_step, hog_channel='ALL', show_windows=True):\n",
    "    \n",
    "    draw_img = np.copy(img)\n",
    "    img = img.astype(np.float32)/255\n",
    "    \n",
    "    img_roi = img[ystart:ystop,:,:]\n",
    "    #ctrans_tosearch = convert_color(img_tosearch, conv='RGB2YCrCb')\n",
    "    if scale != 1:\n",
    "        imshape = img_roi.shape\n",
    "        img_roi = cv2.resize(img_roi, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "    x_size = img_roi.shape[1]\n",
    "    y_size = img_roi.shape[0]\n",
    "    \n",
    "    # Define blocks and steps as above\n",
    "    #Number of Cells\n",
    "    nxcells = x_size // pix_per_cell\n",
    "    nycells = y_size // pix_per_cell\n",
    "    #Number of Blocks\n",
    "    nxblocks = (nxcells - cell_per_block) + 1 ##(W-F)/S + 1, W=ncells, F=cell_per_block, S=Stride=1\n",
    "    nyblocks = (nycells - cell_per_block) + 1 \n",
    "    #nxblocks = (x_size // pix_per_cell)-1  \n",
    "    #nyblocks = (y_size // pix_per_cell)-1 \n",
    "    nfeat_per_block = orient * (cell_per_block**2)\n",
    "    \n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = ((window // pix_per_cell) - cell_per_block) + 1\n",
    "    \n",
    "    #We have 8 cells per 64 pixels\n",
    "    #Each cell step (8 pixels), refers to a move of 1/8 or overlap of 7/8=87.5% overlap\n",
    "    #A step of 2 cells, means we slide windows by 16 pixels, overlap of 75%.\n",
    "    #cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    #Number of steps we can take over HoG array\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog_array = extract_features(img_roi, h_cspace=h_colorspace, s_cspace=s_colorspace, orient=orient, \n",
    "                           pix_per_cell=pix_per_cell, cell_per_block=cell_per_block,\n",
    "                           hog_channel=hog_channel, feature_vec=False, vis=False)\n",
    "    #The hog_array we get has a shape 3XnyblocksXnxblocksXcell_per_blockXcell_per_blockXorientations\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_features = hog_array[:, ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            \n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(img_roi[ytop:ytop+window, xleft:xleft+window, :], (64,64))\n",
    "            \n",
    "            # Get color features\n",
    "            spatial_features = calc_bin_spatial_features(subimg, s_colorspace)\n",
    "            #hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "            \n",
    "            if show_windows:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                color = np.random.randint(0, 256, (3,))\n",
    "                cv2.rectangle(draw_img,(xbox_left, ytop_draw+ystart), \n",
    "                              (xbox_left+win_draw, ytop_draw+win_draw+ystart),color,6) \n",
    "            # Scale features and make a prediction\n",
    "            test_features = X_scaler.transform(np.hstack((hog_features, spatial_features)).reshape(1, -1))\n",
    "            #test_features = X_scaler.transform(hog_features.reshape(1,-1))\n",
    "            #test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1))    \n",
    "            test_prediction = svc.predict(test_features)\n",
    "            \n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                #color = np.random.randint(0, 256, (3,))\n",
    "                cv2.rectangle(draw_img,(xbox_left, ytop_draw+ystart),\n",
    "                              (xbox_left+win_draw, ytop_draw+win_draw+ystart),(0,0,255),6) \n",
    "             \n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(738L, 1280L, 3L)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('test_images/test8.jpg') \n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_cars_1(img):\n",
    "    window = 64\n",
    "    ystart = 400\n",
    "    ystop = ystart + (window*3)\n",
    "    scale = 1 #Steps 5\n",
    "    t1 = time.time()\n",
    "    out_img = find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell,\n",
    "                        cell_per_block, spatial_size, cells_per_step=5, hog_channel=hog_channel, show_windows=False)\n",
    "    return out_img\n",
    "#print(\"Time taken\", time.time()-t1)\n",
    "#plt.imshow(cv2.cvtColor(out_img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_cars_2(img):\n",
    "    ystart = 400 + (window)\n",
    "    ystop = ystart + (window*6)\n",
    "    scale = 1.8 #steps 4\n",
    "\n",
    "    t1 = time.time()\n",
    "    out_img = find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell,\n",
    "                        cell_per_block, spatial_size, cells_per_step=6, hog_channel=hog_channel, show_windows=False)\n",
    "    #print(\"Time taken\", time.time()-t1)\n",
    "    return out_img\n",
    "#plt.imshow(cv2.cvtColor(out_img, cv2.COLOR_BGR2RGB))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine(img1, img2):\n",
    "    return (cv2.addWeighted(img1, 0.5, img2, 0.5, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time taken ', 8.181999921798706)\n",
      "('Time taken ', 17.19099998474121)\n",
      "('Time taken ', 25.91599988937378)\n",
      "('Time taken ', 34.86199998855591)\n",
      "('Time taken ', 43.64199995994568)\n",
      "('Time taken ', 52.36500000953674)\n",
      "('Time taken ', 60.88199996948242)\n",
      "('Time taken ', 69.63800001144409)\n",
      "('Time taken ', 78.32999992370605)\n",
      "('Time taken ', 87.35199999809265)\n",
      "('Time taken ', 98.34299993515015)\n",
      "('Time taken ', 108.32200002670288)\n",
      "('Time taken ', 116.92499995231628)\n",
      "('Time taken ', 125.66599988937378)\n",
      "('Time taken ', 134.3769998550415)\n",
      "('Time taken ', 143.1729998588562)\n",
      "('Time taken ', 152.06299996376038)\n",
      "('Time taken ', 160.74399995803833)\n",
      "('Time taken ', 169.41100001335144)\n",
      "('Time taken ', 178.1289999485016)\n",
      "('Time taken ', 187.15199995040894)\n",
      "('Time taken ', 195.83899998664856)\n",
      "('Time taken ', 204.41100001335144)\n",
      "('Time taken ', 213.07800006866455)\n",
      "('Time taken ', 222.07500004768372)\n",
      "('Time taken ', 230.63299989700317)\n",
      "('Time taken ', 239.50799989700317)\n",
      "('Time taken ', 248.3550000190735)\n",
      "('Time taken ', 257.1949999332428)\n",
      "('Time taken ', 266.08200001716614)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('project_video.mp4')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "recorder = cv2.VideoWriter('out.mp4', fourcc, 30, (1280, 720))\n",
    "t1 = time.time()\n",
    "cnt = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cnt+=1;\n",
    "        #print(\"cnt\", cnt)\n",
    "        if cnt == 300:\n",
    "            break\n",
    "        if cnt%10 == 0:\n",
    "            print('Time taken ', time.time()-t1)\n",
    "        f1 = find_cars_1(frame)\n",
    "        f2 = find_cars_2(frame)\n",
    "        #print(f1.shape)\n",
    "        #print(f2.shape)\n",
    "        img_boxes = combine(find_cars_1(frame), find_cars_2(frame))\n",
    "        recorder.write(img_boxes)\n",
    "    else:\n",
    "        break\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "print('Time taken ', time.time()-t1)\n",
    "cap.release()\n",
    "recorder.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
